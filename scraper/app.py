# app.py
import json
import logging
import os
import threading
import time
from datetime import datetime, timezone
from math import radians, sin, cos, sqrt, atan2
from flask import Flask, jsonify, request
from flask_cors import CORS
from google.cloud import aiplatform, firestore
from google.oauth2 import service_account
from peft import PeftConfig, PeftModel
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline
from vertexai.preview.generative_models import GenerativeModel
from webdriver_manager.chrome import ChromeDriverManager
import requests

# è¨­å®šlogging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.FileHandler("app.log"), logging.StreamHandler()],
)

PROJECT_ID = "data-model-lecture"
GOOGLE_MAPS_API_KEY = os.getenv('VITE_GOOGLE_MAPS_API_KEY', 'YOUR_GOOGLE_MAPS_API_KEY')
GOOGLE_APPLICATION_CREDENTIALS = os.getenv('GOOGLE_APPLICATION_CREDENTIALS', 'YOUR_GOOGLE_APPLICATION_CREDENTIALS')

app = Flask(__name__)
CORS(app)

# export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/service-account-file.json" -> ç’°å¢ƒè®Šæ•¸ mac
# set GOOGLE_APPLICATION_CREDENTIALS=C:\Users\username\Downloads\your-service-account-file.json -> ç’°å¢ƒè®Šæ•¸ windows
credentials = service_account.Credentials.from_service_account_file(
    GOOGLE_APPLICATION_CREDENTIALS
)

# åˆå§‹åŒ– AI Platformï¼Œå‚³éæ†‘è­‰
aiplatform.init(project=PROJECT_ID, credentials=credentials, location="us-central1")

# åˆå§‹åŒ– Firestore å®¢æˆ¶ç«¯
db = firestore.Client(
    project=PROJECT_ID, credentials=credentials, database="dm-firestore"
)

# ç”¨æ–¼å„²å­˜çˆ¬èŸ²ç‹€æ…‹
scraping_status = {}
# ç”¨æ–¼å„²å­˜æ­£åœ¨åŸ·è¡Œçš„ç·šç¨‹
active_threads = {}


def save2json(dir_name: str, file_name: str, reviews: list | dict):
    """å°‡æ•¸æ“šä¿å­˜åˆ°æœ¬åœ° JSON æ–‡ä»¶"""
    if not os.path.exists(dir_name):
        os.makedirs(dir_name)

    file_name = os.path.join(dir_name, file_name)
    with open(file_name, "w", encoding="utf-8") as f:
        json.dump(reviews, f, ensure_ascii=False, indent=4)
    logging.info(f"è©•è«–æ•¸æ“šå·²ä¿å­˜åˆ°: {file_name}")


def build_prompt(context, question):
    """è£½ä½œé¤µçµ¦ Gemini çš„ Prompt"""
    prompt = f"""
    Instructions: Answer the question using the following Context.

    Context: {context}

    Question: {question}
    """
    return prompt


def answer_question_gemini(context, question):
    """ä½¿ç”¨ Gemini æ¨¡å‹å›ç­”å•é¡Œ"""
    prompt = build_prompt(context, question)

    model = GenerativeModel("gemini-1.5-pro-002")
    try:
        response = model.generate_content(
            prompt,
            generation_config={
                "max_output_tokens": 8192,
                "temperature": 0.2,
                "top_p": 0.5,
                "top_k": 10,
            },
            stream=False,
        )
        return response.text
    except Exception as e:
        logging.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")


def upload_reviews_to_firestore(collection_name, reviews):
    """
    å°‡è©•è«–æ•¸æ“šä¸Šå‚³åˆ° Firestore çš„æŒ‡å®šé›†åˆã€‚
    æ¯æ¢è©•è«–ä½œç‚ºä¸€å€‹æ–‡æª”å­˜å„²ã€‚
    """
    try:
        batch = db.batch()
        for review in reviews:
            doc_ref = db.collection(collection_name).document()
            batch.set(doc_ref, review)
        batch.commit()
        logging.info(
            f"æˆåŠŸä¸Šå‚³ {len(reviews)} æ¢è©•è«–åˆ° Firestore é›†åˆ: {collection_name}"
        )
    except Exception as e:
        logging.error(f"ä¸Šå‚³è©•è«–åˆ° Firestore æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise


def upload_analysis_to_firestore(collection_name, keyword, analysis):
    """
    å°‡åˆ†æçµæœä¸Šå‚³åˆ° Firestore çš„æŒ‡å®šé›†åˆä¸­çš„ä¸€å€‹æ–‡æª”ã€‚
    """
    try:
        # å‰µå»ºæˆ–æ›´æ–°ä¸€å€‹æ–‡æª”ç”¨æ–¼å­˜å„²åˆ†æçµæœï¼Œæ–‡æª” ID ç‚ºé—œéµå­—
        doc_ref = db.collection(collection_name).document(keyword)
        doc_ref.set(
            {
                "keyword": keyword,
                "åˆ†æçµæœ": analysis,
                "åˆ†ææ™‚é–“": firestore.SERVER_TIMESTAMP,
                "last_scraped": firestore.SERVER_TIMESTAMP,  # è¨˜éŒ„æœ€å¾Œçˆ¬å–æ™‚é–“
            },
            merge=True,
        )
        logging.info(
            f"æˆåŠŸä¸Šå‚³åˆ†æçµæœåˆ° Firestore é›†åˆ: {collection_name}, æ–‡æª” ID: {keyword}"
        )
    except Exception as e:
        logging.error(f"ä¸Šå‚³åˆ†æçµæœåˆ° Firestore æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise


def should_scrape(keyword, frequency_days=7):
    """
    åˆ¤æ–·æ˜¯å¦éœ€è¦çˆ¬å–è©²é¤å»³çš„è©•è«–ã€‚
    :param keyword: é¤å»³åç¨±
    :param frequency_days: çˆ¬å–é »ç‡ï¼ˆå¤©ï¼‰
    :return: True å¦‚æœéœ€è¦çˆ¬å–ï¼Œå¦å‰‡ False
    """
    doc_ref = db.collection("reviews").document(keyword)
    doc = doc_ref.get()
    if doc.exists:
        last_scraped = doc.to_dict().get("last_scraped")
        if last_scraped:
            last_scraped_time = last_scraped
            current_time = datetime.now(timezone.utc)
            elapsed_days = (current_time - last_scraped_time).days
            return elapsed_days >= frequency_days
    # å¦‚æœæ–‡æª”ä¸å­˜åœ¨æˆ–æ²’æœ‰è¨˜éŒ„ï¼Œå‰‡éœ€è¦çˆ¬å–
    return True


def scrape_google_reviews(
    keyword, driver_path, collection_name="reviews", frequency_days=7
):
    logging.info(f"Start scraping for keyword: {keyword}")
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # service = Service(driver_path)
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    wait = WebDriverWait(driver, 15)

    all_reviews = []

    try:
        if keyword in scraping_status:
            scraping_status[keyword]["status"] = "processing"
            scraping_status[keyword]["message"] = "é€£æ¥åˆ° Google Maps"

        driver.get("https://www.google.com.tw/maps/preview")
        search_box = wait.until(
            EC.presence_of_element_located((By.ID, "searchboxinput"))
        )
        search_box.send_keys(keyword)
        search_box.send_keys(Keys.ENTER)

        review_tab = wait.until(
            EC.element_to_be_clickable((By.XPATH, "//button[.//div[text()='è©•è«–']]"))
        )
        review_tab.click()

        scrollable_div = wait.until(
            EC.presence_of_element_located(
                (
                    By.XPATH,
                    '//*[@id="QA0Szd"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]',
                )
            )
        )

        logging.info("Scrolling to load reviews...")
        for _ in range(10):
            previous_height = driver.execute_script(
                "return arguments[0].scrollHeight;", scrollable_div
            )
            driver.execute_script(
                "arguments[0].scrollTop = arguments[0].scrollHeight;", scrollable_div
            )

            # ç­‰å¾… scrollHeight å¢åŠ ï¼Œä»£è¡¨æœ‰è¼‰å…¥æ–°è©•è«–
            try:
                wait.until(
                    lambda d: driver.execute_script(
                        "return arguments[0].scrollHeight;", scrollable_div
                    )
                    > previous_height
                )
            except:
                # è‹¥è¶…éæŒ‡å®šç§’æ•¸æ²’è®ŠåŒ–ï¼Œè¡¨ç¤ºå·²ç„¡æ›´å¤šè©•è«–å¯ä»¥è¼‰å…¥
                break

        logging.info("Extracting reviews...")
        reviews = driver.find_elements(By.CSS_SELECTOR, "div.jftiEf.fontBodyMedium")
        total_reviews = len(reviews)
        logging.info(f"Total extracted reviews: {total_reviews}")

        if keyword in scraping_status:
            scraping_status[keyword]["total_reviews"] = total_reviews

        # ç²å–ä¸Šæ¬¡çˆ¬å–çš„æœ€æ–°è©•è«–æ™‚é–“
        doc_ref = db.collection("reviews").document(keyword)
        doc = doc_ref.get()
        last_scraped_time = None
        if doc.exists:
            last_scraped = doc.to_dict().get("last_scraped")
            if last_scraped:
                last_scraped_time = last_scraped

        for idx, review in enumerate(reviews, 1):
            try:
                more_button = review.find_elements(
                    By.CSS_SELECTOR, "button.w8nwRe.kyuRq"
                )
                if more_button:
                    more_button[0].click()
                    time.sleep(0.1) # NOTE ä¿®æ”¹æˆ0.1ç§’

                reviewer = review.find_element(By.CSS_SELECTOR, "div.d4r55").text
                rating_element = review.find_element(By.CSS_SELECTOR, "span.kvMYJc")
                rating = (
                    rating_element.get_attribute("aria-label")
                    if rating_element
                    else "ç„¡è©•åˆ†"
                )
                comment = review.find_element(By.CSS_SELECTOR, "span.wiI7pd").text

                # å˜—è©¦ç²å–è©•è«–æ™‚é–“
                try:
                    review_time_str = review.find_element(
                        By.CSS_SELECTOR, "span.rsqaWe"
                    ).text
                    # è§£æè©•è«–æ™‚é–“ï¼ˆæ ¹æ“šå¯¦éš›æ ¼å¼èª¿æ•´ï¼‰
                    review_time = datetime.strptime(
                        review_time_str, "%Y-%m-%d"
                    )  # ç¤ºä¾‹æ ¼å¼
                except Exception:
                    review_time = None

                # å¦‚æœè©•è«–æ™‚é–“æ—©æ–¼ä¸Šæ¬¡çˆ¬å–æ™‚é–“ï¼Œå‰‡è·³é
                if (
                    last_scraped_time
                    and review_time
                    and review_time < last_scraped_time
                ):
                    logging.info(f"è·³éæ—©æ–¼ä¸Šæ¬¡çˆ¬å–çš„è©•è«–: {review_time}")
                    continue

                review_data = {
                    "è©•è«–ç·¨è™Ÿ": idx,
                    "ç”¨æˆ¶": reviewer,
                    "è©•åˆ†": rating,
                    "è©•è«–": comment,
                    "é—œéµå­—": keyword,
                    "æŠ“å–æ™‚é–“": firestore.SERVER_TIMESTAMP,
                    "è©•è«–æ™‚é–“": review_time_str if review_time else None,
                }
                all_reviews.append(review_data)

                if keyword in scraping_status:
                    scraping_status[keyword]["processed_reviews"] = idx

            except Exception as e:
                logging.error(f"è™•ç†ç¬¬ {idx} å‰‡è©•è«–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue

        logging.info("Reviews extracted, uploading to Firestore...")
        # ä¸Šå‚³è©•è«–åˆ° Firestore
        upload_reviews_to_firestore(collection_name, all_reviews)

        logging.info("Reviews uploaded, starting QA analysis...")
        # çˆ¬å®Œä¹‹å¾Œé€²è¡Œ QA åˆ†æå’Œç¸½çµ
        analysis_result = analyze_reviews_with_qa_lora(all_reviews)
        # APIæœƒä½¿ç”¨å¤ªå¤šè³‡æºï¼Œæ‰€ä»¥ä½¿ç”¨ local LLM é…åˆ lora é€²è¡Œåˆ†æ
        # analysis_result = analyze_reviews_with_qa_gemeni(all_reviews)

        logging.info("QA analysis completed, uploading analysis to Firestore...")
        # ä¸Šå‚³åˆ†æçµæœåˆ° Firestore
        upload_analysis_to_firestore(collection_name, keyword, analysis_result)

        if keyword in scraping_status:
            scraping_status[keyword]["status"] = "completed"
            scraping_status[keyword][
                "message"
            ] = f"å®Œæˆï¼Œå…±æ”¶é›† {len(all_reviews)} å‰‡è©•è«–ï¼Œä¸¦ç”¢ç”ŸQAåˆ†æçµæœ"

        logging.info("Scraping and analysis completed.")
    except Exception as e:
        logging.error(f"Error during scraping: {e}")
        if keyword in scraping_status:
            scraping_status[keyword]["status"] = "error"
            scraping_status[keyword]["error"] = str(e)
        raise
    finally:
        driver.quit()


def analyze_reviews_with_qa_lora(reviews):
    logging.info("Analyzing reviews with QA pipeline...")

    # è®“å•é¡Œæœ¬èº«æ›´æ˜ç¢º,å¼•å°æ¨¡å‹çµ¦å‡ºæ›´æº–ç¢ºçš„ç­”æ¡ˆ
    question1 = "æ ¹æ“šé€™æ®µè©•è«–,é€™å®¶é¤å»³å¯¦éš›è¡¨ç¾å¥½çš„åœ°æ–¹æœ‰å“ªäº›?è«‹åˆ—å‡ºå…·é«”çš„å„ªé»ã€‚è‹¥ç„¡å‰‡å›ç­”ã€Œç„¡å„ªé»ã€"
    question2 = "æ ¹æ“šé€™æ®µè©•è«–,é€™å®¶é¤å»³å¯¦éš›è¡¨ç¾ä¸å¥½çš„åœ°æ–¹æœ‰å“ªäº›?è«‹åˆ—å‡ºå…·é«”çš„ç¼ºé»ã€‚è‹¥ç„¡å‰‡å›ç­”ã€Œç„¡ç¼ºé»ã€"
    question3 = (
        "æ ¹æ“šé€™æ®µè©•è«–,æœ‰å“ªäº›å€¼å¾—ä¸€è©¦çš„é¤é»æˆ–ç‰¹è‰²èœ?è«‹åˆ—å‡ºå…·é«”èœåã€‚è‹¥ç„¡å‰‡å›ç­”ã€Œç„¡æ¨è–¦ã€"
    )

    positives = []
    negatives = []
    recommendations = []

    seen_positives = set()
    seen_negatives = set()
    seen_recommendations = set()

    for idx, r in enumerate(reviews, start=1):
        context = r.get("è©•è«–", "")
        if not context:
            continue

        try:
            ans1 = qa_pipeline(question=question1, context=context)
            ans2 = qa_pipeline(question=question2, context=context)
            ans3 = qa_pipeline(question=question3, context=context)

            # åªéæ¿¾é‡è¤‡å…§å®¹å’Œç„¡æ•ˆç­”æ¡ˆ
            if ans1 and ans1["answer"] and ans1["answer"] != "ç„¡å„ªé»":
                if ans1["answer"] not in seen_positives:
                    positives.append(ans1["answer"])
                    seen_positives.add(ans1["answer"])

            if ans2 and ans2["answer"] and ans2["answer"] != "ç„¡ç¼ºé»":
                if ans2["answer"] not in seen_negatives:
                    negatives.append(ans2["answer"])
                    seen_negatives.add(ans2["answer"])

            if ans3 and ans3["answer"] and ans3["answer"] != "ç„¡æ¨è–¦":
                if ans3["answer"] not in seen_recommendations:
                    recommendations.append(ans3["answer"])
                    seen_recommendations.add(ans3["answer"])

            if idx % 10 == 0:
                logging.info(f"QA processed {idx}/{len(reviews)} reviews...")

        except Exception as e:
            logging.error(f"QAè™•ç†ç¬¬ {idx} å‰‡è©•è«–æ™‚å‡ºç¾å•é¡Œ: {e}")
            continue

    # åœ¨é€²è¡Œ GPT ç¸½çµå‰ï¼Œå…ˆé€²è¡Œä¸€æ¬¡ GPT ç¯©é¸
    logging.info("Starting GPT filtering...")

    results = {
        "positives": positives,
        "negatives": negatives,
        "recommendations": recommendations,
    }

    save2json(dir_name="results", file_name="first_result.json", reviews=results)

    filtered_results = filter_with_gemini(positives, negatives, recommendations)

    save2json(dir_name="results", file_name="filtered_result.json", reviews=results)

    logging.info("GPT filtering completed, starting final summary...")
    summary_result = summarize_with_gemini(
        filtered_results["positives"],
        filtered_results["negatives"],
        filtered_results["recommendations"],
    )

    final_result = {"individual_analysis": filtered_results, "summary": summary_result}

    save2json(dir_name="results", file_name="final_result.json", reviews=final_result)

    return final_result


def analyze_reviews_with_qa_gemeni(reviews):
    logging.info("Analyzing reviews with QA pipeline...")

    # è®“å•é¡Œæœ¬èº«æ›´æ˜ç¢º,å¼•å°æ¨¡å‹çµ¦å‡ºæ›´æº–ç¢ºçš„ç­”æ¡ˆ
    context1 = """
        ä½ æ˜¯è©•è«–å¤§å¸«ï¼Œæ“æœ‰æ•¸åå¹´çš„é¤å»³è©•è«–ç¶“é©—ã€‚è«‹ä½ æ ¹æ“šé€™æ®µä½¿ç”¨è€…çµ¦çš„è©•åƒ¹ï¼Œå›ç­”é€™å®¶é¤å»³å¯¦éš›è¡¨ç¾å¥½çš„åœ°æ–¹æœ‰ä»€éº¼ï¼Ÿ
    
        ä»¥ä¸‹æ˜¯ä½ å¿…é ˆéµå®ˆçš„ï¼š
        1. å›ç­”å¿…é ˆæ˜¯æœ‰æ„ç¾©çš„ï¼Œä¸èƒ½æ˜¯ç„¡æ„ç¾©çš„æ–‡å­—ã€‚
        2. åªéœ€è¦ä¸€å€‹ï¼Œä¸”æ§åˆ¶åœ¨20å€‹å­—ä»¥å…§ã€‚
        3. å…·é«”çš„æè¿°ï¼Œä¸è¦å«ç³Šä¸æ¸…ã€å¤ªæ”çµ±ã€‚
        4. è‹¥ç„¡å‰‡å›ç­”ã€Œç„¡å„ªé»ã€
        
        çµ¦ä½ ä¸€å€‹ä¾‹å­ï¼š
        "ç¾©å¤§åˆ©éºµğŸå’ŒæŠ«è–©ğŸ•ç­‰ä¸»é¤åƒ¹ä½éƒ½è½åœ¨350å·¦å³
        æ’é¤åƒæ˜¯ç‰›æ’ã€é¾è¦ğŸ¦åƒ¹ä½æ‰æ¯”è¼ƒé«˜
        æŠ«è–©æ˜¯10å‹çš„ç”¨æ–™å¯¦åœ¨cpå€¼å¾ˆé«˜
        æŠ«è–©ä¸Šçš„èµ·å¸ã€æ˜å¤ªå­éƒ½å¾ˆæ¿ƒéƒ å”æšé›æ˜¯é›è…¿è‚‰ä¹Ÿå¾ˆå¤ å‘³ å¯ä»¥2ï½3äººä¸€èµ·åˆ†ğŸ˜‹
        ä¸»é¤å¯ä»¥+200å…ƒå°±æœ‰å‰èœã€é£²æ–™ã€æ²™æ‹‰ã€ç”œé»å¯ä»¥é¸ğŸ‘ğŸ»"
        
        ä½ éœ€è¦å›ç­”ï¼š
        "æŠ«è–©ç”¨æ–™å¯¦åœ¨cpå€¼å¾ˆé«˜"
    """
    context2 = """
        ä½ æ˜¯è©•è«–å¤§å¸«ï¼Œæ“æœ‰æ•¸åå¹´çš„é¤å»³è©•è«–ç¶“é©—ã€‚è«‹ä½ æ ¹æ“šé€™æ®µä½¿ç”¨è€…çµ¦çš„è©•åƒ¹ï¼Œå›ç­”é€™å®¶é¤å»³å¯¦éš›è¡¨ç¾ã€Œä¸å¥½ã€çš„åœ°æ–¹æœ‰ä»€éº¼ï¼Ÿ
        
        ä»¥ä¸‹æ˜¯ä½ å¿…é ˆéµå®ˆçš„ï¼š
        1. å›ç­”å¿…é ˆæ˜¯æœ‰æ„ç¾©çš„ï¼Œä¸èƒ½æ˜¯ç„¡æ„ç¾©çš„æ–‡å­—ã€‚
        2. åªéœ€è¦ä¸€å€‹ï¼Œä¸”æ§åˆ¶åœ¨20å€‹å­—ä»¥å…§ã€‚
        3. å…·é«”çš„æè¿°ï¼Œä¸è¦å«ç³Šä¸æ¸…ã€å¤ªæ”çµ±ã€‚
        4. è‹¥ç„¡å‰‡å›ç­”ã€Œç„¡ç¼ºé»ã€
        
        çµ¦ä½ ä¸€å€‹ä¾‹å­ï¼š
        "ç¾©å¤§åˆ©éºµğŸå’ŒæŠ«è–©ğŸ•ç­‰ä¸»é¤åƒ¹ä½éƒ½è½åœ¨350å·¦å³
        æ’é¤åƒæ˜¯ç‰›æ’ã€é¾è¦ğŸ¦åƒ¹ä½æ‰æ¯”è¼ƒé«˜
        æŠ«è–©æ˜¯10å‹çš„ç”¨æ–™å¯¦åœ¨cpå€¼å¾ˆé«˜
        æŠ«è–©ä¸Šçš„èµ·å¸ã€æ˜å¤ªå­éƒ½å¾ˆæ¿ƒéƒ å”æšé›æ˜¯é›è…¿è‚‰ä¹Ÿå¾ˆå¤ å‘³ å¯ä»¥2ï½3äººä¸€èµ·åˆ†ğŸ˜‹
        ä¸»é¤å¯ä»¥+200å…ƒå°±æœ‰å‰èœã€é£²æ–™ã€æ²™æ‹‰ã€ç”œé»å¯ä»¥é¸ğŸ‘ğŸ»"
        
        ä½ éœ€è¦å›ç­”ï¼š
        "æ’é¤åƒ¹ä½é«˜"
    """
    context3 = """
        ä½ æ˜¯è©•è«–å¤§å¸«ï¼Œæ“æœ‰æ•¸åå¹´çš„é¤å»³è©•è«–ç¶“é©—ã€‚è«‹ä½ æ ¹æ“šé€™æ®µä½¿ç”¨è€…çµ¦çš„è©•åƒ¹ï¼Œæœ‰å“ªäº›å€¼å¾—ä¸€è©¦çš„é¤é»æˆ–ç‰¹è‰²èœï¼Ÿ
        
        ä»¥ä¸‹æ˜¯ä½ å¿…é ˆéµå®ˆçš„ï¼š
        1. å›ç­”å¿…é ˆæ˜¯æœ‰æ„ç¾©çš„ï¼Œä¸èƒ½æ˜¯ç„¡æ„ç¾©çš„æ–‡å­—ã€‚
        2. åªéœ€è¦ä¸€å€‹ï¼Œä¸”æ§åˆ¶åœ¨20å€‹å­—ä»¥å…§ã€‚
        3. åˆ—å‡ºå…·é«”èœåï¼Œä¸è¦å«ç³Šä¸æ¸…ã€å¤ªæ”çµ±ã€‚
        4. è‹¥ç„¡å‰‡å›ç­”ã€Œç„¡æ¨è–¦ã€ã€‚
        
        çµ¦ä½ ä¸€å€‹ä¾‹å­ï¼š
        "ç¾©å¤§åˆ©éºµğŸå’ŒæŠ«è–©ğŸ•ç­‰ä¸»é¤åƒ¹ä½éƒ½è½åœ¨350å·¦å³
        æ’é¤åƒæ˜¯ç‰›æ’ã€é¾è¦ğŸ¦åƒ¹ä½æ‰æ¯”è¼ƒé«˜
        æŠ«è–©æ˜¯10å‹çš„ç”¨æ–™å¯¦åœ¨cpå€¼å¾ˆé«˜
        æŠ«è–©ä¸Šçš„èµ·å¸ã€æ˜å¤ªå­éƒ½å¾ˆæ¿ƒéƒ å”æšé›æ˜¯é›è…¿è‚‰ä¹Ÿå¾ˆå¤ å‘³ å¯ä»¥2ï½3äººä¸€èµ·åˆ†ğŸ˜‹
        ä¸»é¤å¯ä»¥+200å…ƒå°±æœ‰å‰èœã€é£²æ–™ã€æ²™æ‹‰ã€ç”œé»å¯ä»¥é¸ğŸ‘ğŸ»"
        
        ä½ éœ€è¦å›ç­”ï¼š
        "ç¾©å¤§åˆ©éºµå’ŒæŠ«è–©"
    """

    positives = []  # å„ªé»
    negatives = []  # ç¼ºé»
    recommendations = []  # æ¨è–¦

    seen_positives = set()  # ç”¨æ–¼éæ¿¾é‡è¤‡å…§å®¹
    seen_negatives = set()  # ç”¨æ–¼éæ¿¾é‡è¤‡å…§å®¹
    seen_recommendations = set()  # ç”¨æ–¼éæ¿¾é‡è¤‡å…§å®¹

    for idx, r in enumerate(reviews, start=1):
        question = r.get("è©•è«–", "")
        if not question:
            continue

        try:
            ans1 = answer_question_gemini(context=context1, question=question)

            ans2 = answer_question_gemini(context=context2, question=question)

            ans3 = answer_question_gemini(context=context3, question=question)

            # åªéæ¿¾é‡è¤‡å…§å®¹å’Œç„¡æ•ˆç­”æ¡ˆ
            if ans1 and ans1 != "ç„¡å„ªé»":
                if ans1 not in seen_positives:
                    positives.append(ans1)
                    seen_positives.add(ans1)

            if ans2 and ans2 != "ç„¡ç¼ºé»":
                if ans2 not in seen_negatives:
                    negatives.append(ans2)
                    seen_negatives.add(ans2)

            if ans3 and ans3 != "ç„¡æ¨è–¦":
                if ans3 not in seen_recommendations:
                    recommendations.append(ans3)
                    seen_recommendations.add(ans3)

            if idx % 10 == 0:
                logging.info(f"QA processed {idx}/{len(reviews)} reviews...")

        except Exception as e:
            logging.error(f"QAè™•ç†ç¬¬ {idx} å‰‡è©•è«–æ™‚å‡ºç¾å•é¡Œ: {e}")
            continue

    # åœ¨é€²è¡Œ gemini ç¸½çµå‰ï¼Œå…ˆé€²è¡Œä¸€æ¬¡ gemini ç¯©é¸
    logging.info("Starting gemini filtering...")

    # data ={
    #     "positives": positives,
    #     "negatives": negatives,
    #     "recommendations": recommendations
    # }
    # print(data)

    # with open("first.json", "w", encoding='utf-8') as f:
    #     json.dump(data, f, ensure_ascii=False, indent=4)

    filtered_results = filter_with_gemini(positives, negatives, recommendations)  # json

    logging.info("gemini filtering completed, starting final summary...")
    summary_result = summarize_with_gemini(  # str
        filtered_results["positives"],
        filtered_results["negatives"],
        filtered_results["recommendations"],
    )

    return {"individual_analysis": filtered_results, "summary": summary_result}


def filter_with_gemini(positives, negatives, recommendations):
    context = (
        context
    ) = """
        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é¤å»³è©•è«–åˆ†æå°ˆå®¶ã€‚
        
        è«‹ step by step ä»”ç´°åˆ†æé¤å»³è©•è«–ä¸­æå–å‡ºçš„å…§å®¹ï¼Œä¸¦é€²è¡ŒäºŒæ¬¡ç¯©é¸ï¼Œç¢ºä¿å…§å®¹çš„æº–ç¢ºæ€§å’Œç›¸é—œæ€§ã€‚
        
        éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
        1. ç§»é™¤ä¸ç›¸é—œæˆ–é‡è¤‡çš„å…§å®¹
        2. æ•´åˆç›¸ä¼¼çš„æè¿°
        3. ç§»é™¤æ¨¡ç³Šä¸æ¸…çš„è©•åƒ¹ï¼Œä¸è¦ä¿ç•™æ²’æœ‰æ„ç¾©çš„æ–‡å­—
        4. è«‹å‹™å¿…ä»¥ JSON æ ¼å¼è¿”å›çµæœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        {
            "positives": list[str],
            "negatives": list[str],
            "recommendations": list[str]
        }
        5. json å›è¦†æ™‚ï¼Œä¸è¦æœ‰å¤šé¤˜çš„å­—é«”ï¼Œåƒæ˜¯ "json"ã€æ›è¡Œç¬¦è™Ÿç­‰
    """

    question = f"""
    è«‹åƒè€ƒä»¥ä¸‹å„ªç¼ºé»å’Œæ¨è–¦çš„åŸå§‹è³‡æ–™ï¼Œä¸¦æ ¹æ“šä¸Šè¿°è¦æ±‚é€²è¡Œåˆ†æå’Œæ•´ç†ã€‚
        original positives:
        {json.dumps(positives, ensure_ascii=False)}

        original negatives:
        {json.dumps(negatives, ensure_ascii=False)}

        original recommendations:
        {json.dumps(recommendations, ensure_ascii=False)}
    """

    try:
        response = answer_question_gemini(context=context, question=question)
        response = response.strip()
        logging.info("gemini filtering completed.")
        return json.loads(response)
    except Exception as e:
        logging.error(f"gemini ç¯©é¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {
            "positives": positives,
            "negatives": negatives,
            "recommendations": recommendations,
        }


def summarize_with_gemini(positives, negatives, recommendations):
    context = """
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é¤å»³è©•è«–å®¶ï¼Œæ“æœ‰è±å¯Œçš„ç¶“é©—ã€‚ç”¨ä¸€æ®µè©±ç¸½çµä¸€ä¸‹æ•´é«”æ„Ÿå—ï¼Œé€™å®¶é¤å»³é©åˆä»€éº¼æ¨£çš„æ¶ˆè²»è€…ï¼Œæœ‰å“ªäº›å€¼å¾—æ”¹é€²çš„åœ°æ–¹ã€‚
        
        è¦æ±‚ï¼š
        1. è«‹ç›´æ¥é™³è¿°åˆ†æçµæœ
        2. ä¿æŒå°ˆæ¥­å®¢è§€çš„èªæ°£
        3. é‡é»æ‘˜è¦é¤å»³çš„ç‰¹è‰²å’Œæœå‹™
        4. æ•´é«”è©•åˆ†(æ»¿åˆ†5åˆ†)è«‹å…ˆåˆ—å‡ºï¼Œä¸¦å†è‡ªç„¶åœ°èå…¥æè¿°ä¸­
        5. ä¸è¦ä½¿ç”¨ã€Œå¾è©•è«–ä¸­å¯ä»¥çœ‹å‡ºã€ä¹‹é¡çš„å¼•å°èªã€‚è©•åˆ†è«‹å…ˆå–®ç¨åˆ—å‡ºï¼Œä¸¦åŒæ™‚æ•´åˆåœ¨å…§å®¹ä¸­ã€‚
        
        æˆ‘çµ¦ä½ ä¸€å€‹å›ç­”ä¾‹å¦‚ï¼š
        "è©•åˆ†ï¼š4/5\n\né€™å®¶ä½æ–¼æœ¨æ–°è·¯çš„ç¾©å¤§åˆ©æ–™ç†é¤å»³æ“æœ‰å¤šæ¨£åŒ–çš„èœå–®ï¼ŒåŒ…æ‹¬å¥—é¤å’Œæ—©åˆé¤é¸é …ï¼Œå°¤å…¶æ¨è–¦å¦‚ç‰›æ’ã€ç¾©å¤§åˆ©éºµå’Œçƒ¤é£¯ç­‰ä¸»èœã€‚ç‰¹è‰²ç”œé»å¦‚ææ‹‰ç±³è˜‡å’Œå¸ƒæœ—å°¼è›‹ç³•ä¹Ÿæ·±å—å¥½è©•ã€‚ç’°å¢ƒæ–¹é¢ï¼Œè£æ½¢å¤å…¸ä¸”å¯Œæœ‰æ­å¼é¢¨æ ¼ï¼Œæä¾›äº†ä¸€å€‹æ°£æ°›ä½³ä¸”èˆ’é©çš„ç”¨é¤ç’°å¢ƒï¼Œé©åˆå¤šäººèšé¤ã€‚\n\nå„˜ç®¡æœå‹™æ…‹åº¦æ™®éè¦ªåˆ‡ï¼Œä½†å­˜åœ¨æœå‹™ç”Ÿé›£ä»¥æ‰¾åˆ°çš„å•é¡Œï¼Œå¯èƒ½æœƒå½±éŸ¿é¡§å®¢çš„ç”¨é¤é«”é©—ã€‚æ­¤å¤–ï¼Œéƒ¨åˆ†é¤é»å¦‚è±¬è‚‰ä¸²å’Œæ²™æ‹‰çš„å£å‘³æœ‰å¾…æå‡ã€‚é¤å»³ä½ç½®å°æŸäº›é¡§å®¢ä¾†èªªå¯èƒ½ä¸å¤ªæ–¹ä¾¿ã€‚\n\nç¸½é«”ä¾†èªªï¼Œé€™å®¶é¤å»³å› å…¶ç¾å‘³çš„é£Ÿç‰©ã€å¤šæ¨£çš„é¸æ“‡å’Œå„ªé›…çš„ç’°å¢ƒå—åˆ°æ¨å´‡ã€‚å°æ–¼å°‹æ±‚ç¾å‘³ç¾©å¤§åˆ©æ–™ç†å’Œæ„‰æ‚…ç”¨é¤ç’°å¢ƒçš„é¡§å®¢ä¾†èªªï¼Œæ˜¯ä¸€å€‹ä¸éŒ¯çš„é¸æ“‡ã€‚ç„¶è€Œï¼Œå»ºè­°é¤å»³æ”¹é€²æœå‹™æ•ˆç‡å’Œéƒ¨åˆ†èœå“çš„å“è³ªï¼Œä»¥æå‡é¡§å®¢æ»¿æ„åº¦ã€‚æ€»ä½“è€Œè¨€ï¼Œé€™å®¶é¤å»³éå¸¸é©åˆå–œæ­¡å˜—è©¦é«˜å“è³ªç¾©å¤§åˆ©èœå’Œäº«å—ç¾éº—ç’°å¢ƒçš„é¡§å®¢ã€‚"
    """
    questions = f"""
    ç¾åœ¨ä½ å¿…é ˆæ ¹æ“šä»¥ä¸‹é€™äº›è³‡è¨Šï¼Œè«‹ç‚ºé¤å»³è©•è«–æä¾›ä¸€å€‹ç°¡æ½”çš„ç¸½çµã€‚
    
        å„ªé»ï¼š
        {json.dumps(positives, ensure_ascii=False)}

        ç¼ºé»ï¼š
        {json.dumps(negatives, ensure_ascii=False)}

        æ¨è–¦å¿…é»ï¼š
        {json.dumps(recommendations, ensure_ascii=False)}
    """

    try:
        answer = answer_question_gemini(context=context, question=questions)
        logging.info("gemini summarization completed.")
    except Exception as e:
        logging.error(f"gemini ç¸½çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        answer = f"gemini ç¸½çµæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"

    return answer


@app.route("/api/reviews/<keyword>/status", methods=["GET"])
def get_status(keyword):
    try:
        doc_ref = db.collection("reviews").document(keyword)
        doc = doc_ref.get()
        if doc.exists:
            # analysis = doc.to_dict().get("åˆ†æçµæœ", {})
            scraping_status[keyword]["status"] = "completed"
            return jsonify(scraping_status[keyword])
        return jsonify({"status": "not_found", "message": "No scraping job found"}), 404
    except Exception as e:
        logging.error(f"Error getting analysis for {keyword}: {e}")
        return jsonify({"error": str(e)}), 500
    finally:
        if keyword in scraping_status:
            return jsonify(scraping_status[keyword])
        return jsonify({"status": "not_found", "message": "No scraping job found"}), 404


@app.route("/api/scrape-reviews", methods=["POST"])
def start_scrape():
    try:
        data = request.json
        keyword = data.get("keyword")

        if not keyword:
            logging.warning("No keyword provided in request")
            return jsonify({"error": "No keyword provided"}), 400

        # åˆ¤æ–·æ˜¯å¦éœ€è¦çˆ¬å–
        if not should_scrape(keyword):
            # å¦‚æœæ²’æœ‰ç‹€æ…‹ï¼Œå‰‡æœƒç„¡æ³•è§¸ç™¼å‰ç«¯æŠ“å–è³‡è¨Š
            scraping_status[keyword] = {
                "status": "completed",
                "message": "æœªé”åˆ°çˆ¬å–é »ç‡",
                "total_reviews": 0,
                "processed_reviews": 0,
            }
            logging.info(f"ä¸éœ€è¦çˆ¬å– {keyword}ï¼Œå› ç‚ºæœªé”åˆ°çˆ¬å–é »ç‡")
            return (
                jsonify(
                    {
                        "message": "Scraping not needed at this time",
                        "status": "not_required",
                    }
                ),
                200,
            )

        # åˆå§‹åŒ–ç‹€æ…‹
        scraping_status[keyword] = {
            "status": "initializing",
            "message": "åˆå§‹åŒ–ä¸­",
            "total_reviews": 0,
            "processed_reviews": 0,
        }

        logging.info(f"Starting scrape thread for keyword: {keyword}")
        # é–‹å§‹çˆ¬èŸ²ç·šç¨‹
        thread = threading.Thread(
            target=scrape_google_reviews,
            args=(
                keyword,
                "scraper/chromedriver-win32/chromedriver-win64/chromedriver.exe",  # NOTE ç¢ºä¿ chromedriver è·¯å¾‘æ­£ç¢º
                "reviews",
            ),
        )
        active_threads[keyword] = thread
        thread.start()

        return jsonify({"message": "Scraping started", "status": "processing"})

    except Exception as e:
        logging.error(f"Error when starting scrape: {e}")
        return jsonify({"error": str(e)}), 500

def calculate_distance(loc1, loc2):
    """
    ä½¿ç”¨ Haversine å…¬å¼è¨ˆç®—å…©é»ä¹‹é–“çš„è·é›¢ï¼ˆç±³ï¼‰
    """
    R = 6371e3  # åœ°çƒåŠå¾‘ï¼ˆç±³ï¼‰
    phi1 = radians(loc1['lat'])
    phi2 = radians(loc2['lat'])
    delta_phi = radians(loc2['lat'] - loc1['lat'])
    delta_lambda = radians(loc2['lng'] - loc1['lng'])

    a = sin(delta_phi / 2) ** 2 + cos(phi1) * cos(phi2) * sin(delta_lambda / 2) ** 2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))

    distance = R * c
    return round(distance)

@app.route("/api/reviews/<keyword>", methods=["GET"])
def get_reviews(keyword):
    try:
        reviews_ref = db.collection("reviews")
        print("keyword: " + keyword)
        query = reviews_ref.where("`é—œéµå­—`", "==", keyword).stream()

        reviews = []
        for doc in query:
            review = doc.to_dict()
            # ç§»é™¤ Firestore å…§éƒ¨çš„å­—æ®µ
            review.pop("é—œéµå­—", None)
            review.pop("æŠ“å–æ™‚é–“", None)
            reviews.append(review)

        if reviews:
            return jsonify(reviews)
        return jsonify([]), 404
    except Exception as e:
        logging.error(f"Error getting reviews for {keyword}: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/reviews/<keyword>_analysis", methods=["GET"])
def get_analysis(keyword):
    try:
        doc_ref = db.collection("reviews").document(keyword)
        doc = doc_ref.get()
        if doc.exists:
            analysis = doc.to_dict().get("åˆ†æçµæœ", {})
            return jsonify(analysis)
        return jsonify({"error": "Analysis not found"}), 404
    except Exception as e:
        logging.error(f"Error getting analysis for {keyword}: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/nearby-restaurants', methods=['GET'])
def get_nearby_restaurants():
    lat = request.args.get('lat')
    lng = request.args.get('lng')
    radius = request.args.get('radius', 1500)

    if not lat or not lng:
        return jsonify({'error': 'Missing latitude or longitude'}), 400

    try:
        lat = float(lat)
        lng = float(lng)
        radius = int(radius)
    except ValueError:
        return jsonify({'error': 'Invalid latitude, longitude, or radius format'}), 400

    url = (
        f'https://maps.googleapis.com/maps/api/place/nearbysearch/json'
        f'?location={lat},{lng}&radius={radius}&type=restaurant&key={GOOGLE_MAPS_API_KEY}'
    )

    try:
        response = requests.get(url)
        data = response.json()

        if data.get('status') != 'OK':
            raise Exception(f"Google Places API error: {data.get('status')}")

        results = []
        for place in data.get('results', []):
            place_lat = place['geometry']['location']['lat']
            place_lng = place['geometry']['location']['lng']
            distance = calculate_distance(
                {'lat': lat, 'lng': lng},
                {'lat': place_lat, 'lng': place_lng}
            )
            results.append({
                'id': place['place_id'],
                'name': place['name'],
                'address': place.get('vicinity', ''),
                'lat': place_lat,
                'lng': place_lng,
                'distance': distance
            })

        return jsonify(results)
    except Exception as error:
        print(error)
        return jsonify({'error': 'Failed to fetch restaurants'}), 500

if __name__ == "__main__":
    # è¨­å®šQAæ¨¡å‹è·¯å¾‘ï¼ˆè«‹ç¢ºèªæ¨¡å‹æ–‡ä»¶åœ¨æ­¤è·¯å¾‘ä¸‹ï¼‰
    model_path = r"scraper/lora_qa_model_new/lora_qa_model_new"

    # ä½¿ç”¨PEFTå¾LoRAæ¨¡å‹ä¸­å–config
    logging.info("Loading PEFT config...")
    peft_config = PeftConfig.from_pretrained(model_path)

    logging.info("Loading base model and tokenizer...")
    base_tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)
    base_model = AutoModelForQuestionAnswering.from_pretrained(
        peft_config.base_model_name_or_path
    )

    logging.info("Loading LoRA weights...")
    model = PeftModel.from_pretrained(base_model, model_path)
    tokenizer = base_tokenizer

    logging.info("Initializing QA pipeline...")
    qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)
    app.run(debug=True, port=5000)
